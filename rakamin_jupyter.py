{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.295802Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.296277Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.302173Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.296241Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.300720Z\"}}\nimport dask.dataframe as dd\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.304634Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.305090Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.335507Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.305047Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.334270Z\"}}\nimport os\ntry:\n    import pycaret\n    import pyspark\nexcept:\n    os.system(\"pip install -q pycaret\")\n    os.system('pip install -q pyspark')\n    os.system('pip install -q graphviz')\n    os.system('pip install -q dtreeviz')\n    os.system(\"pip install -q pydot\")\n    os.system(\"pip install -q autoimpute\")\n    os.system(\"pip install -q ydata-profiling\")\n    pass\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.339748Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.340090Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.357831Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.340062Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.356640Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [markdown]\n# # Add Function\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.359456Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.359793Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.367452Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.359765Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.366237Z\"}}\nimport seaborn as sns\n\ndef plot_feature_importance(model, feature_names, figsize=(8, 6), orientation='horizontal', ascending=False, n=None):\n    feat_importances = pd.Series(model.feature_importances_, index=feature_names)\n    if n is None:\n        n = len(feat_importances)\n    if(ascending):\n        feat_importances.sort_values().nlargest(n).plot(kind='barh', figsize=figsize, orientation=orientation)\n    else:\n        feat_importances.sort_values().nsmallest(n).plot(kind='barh', figsize=figsize, orientation=orientation)\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.369687Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.370029Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.383823Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.370000Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.382744Z\"}}\nimport pandas as pd\n\ndef extract_datetime(df, columns=None, year=True, month=True, day=True, hour=True, minute=True, second=True, drop=False):\n    if columns is None:\n        columns = df.columns\n\n    for column_name in columns:\n        if df[column_name].dtype == 'object':\n            try:\n                df[column_name] = pd.to_datetime(df[column_name])\n            except ValueError:\n                pass\n\n        if df[column_name].dtype == 'datetime64[ns]':\n            if year:\n                df[column_name + '_year'] = df[column_name].dt.year\n            if month:\n                df[column_name + '_month'] = df[column_name].dt.month\n            if day:\n                df[column_name + '_day'] = df[column_name].dt.day\n            if hour:\n                df[column_name + '_hour'] = df[column_name].dt.hour\n            if minute:\n                df[column_name + '_minute'] = df[column_name].dt.minute\n            if second:\n                df[column_name + '_second'] = df[column_name].dt.second\n\n            if drop:\n                df.drop(column_name, axis=1, inplace=True)\n\n    return df\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.548840Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.549207Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.564542Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.549179Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.563604Z\"}}\nimport re\n\ndef tree_to_dot(tree_str, feature_names=None):\n    dot_str = 'digraph Tree {\\nnode [shape=box] ;\\n'\n    lines = tree_str.split('\\n')\n    stack = []\n    node_id = 0\n    for line in lines:\n        if line.strip() == '':\n            continue\n        depth = len(re.findall('\\s\\s', line))\n        line = line.strip()\n        if 'If' in line:\n            if ' <= ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' <= ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} <= {value}\"] ;\\n'\n            elif ' > ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' > ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} > {value}\"] ;\\n'\n            elif ' in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} in {value}\"] ;\\n'\n            elif ' not in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' not in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} not in {value}\"] ;\\n'\n            if len(stack) > 0:\n                dot_str += f'{stack[-1]} -> {node_id} [labeldistance=2.5, labelangle=45, headlabel=\"True\"];\\n'\n            stack.append(node_id)\n            node_id += 1\n        elif 'Predict' in line:\n            value = re.findall(r': ([^ ]+)', line)[0]\n            dot_str += f'{node_id} [label=\"Predict: {value}\", fillcolor=\"#e5813900\"] ;\\n'\n            dot_str += f'{stack[-1]} -> {node_id} ;\\n'\n            node_id += 1\n        elif 'Else' in line:\n            stack = stack[:depth]\n            if ' > ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' > ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} > {value}\"] ;\\n'\n            elif ' <= ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' <= ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} <= {value}\"] ;\\n'\n            elif ' not in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' not in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} not in {value}\"] ;\\n'\n            elif ' in ' in line:\n                feature, value = re.findall(r'\\(([^)]+)\\)', line)[0].split(' in ')\n                feature_name = feature_names[int(feature)] if feature_names else f'X[{feature}]'\n                dot_str += f'{node_id} [label=\"{feature_name} in {value}\"] ;\\n'\n            dot_str += f'{stack[-1]} -> {node_id} [labeldistance=2.5, labelangle=-45, headlabel=\"False\"];\\n'\n            stack.append(node_id)\n            node_id += 1\n    dot_str += '}'\n    return dot_str\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.565986Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.567054Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.584870Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.567016Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.583683Z\"}}\nfrom pyspark.ml.classification import DecisionTreeClassificationModel\n# from pyspark.ml.tree import Node\n\ndef extract_rules(node, feature_names, rule_string=\"\"):\n    if node.numChildren() == 0:\n        return rule_string + \"Predict: \" + str(node.prediction())\n\n    feature = node.split().featureIndex()\n    threshold = node.split().threshold()\n    feature_name = feature_names[feature]\n    condition = \" <= \" if node.split().leftSide() else \" > \"\n    rule_string += \"If (\" + feature_name + condition + str(threshold) + \")\\n\"\n\n    left_child_rule = extract_rules(node.leftChild(), feature_names, rule_string + \"  \")\n    right_child_rule = extract_rules(node.rightChild(), feature_names, rule_string + \"  \")\n    return left_child_rule + \"\\n\" + right_child_rule\n\ndef decision_tree_to_dot_data(dt_model, feature_names):\n    tree_model = dt_model.stages[-1]\n    root_node = tree_model._call_java(\"rootNode\")\n\n    dot_data = \"digraph Tree {\\n\"\n    dot_data += \"node [shape=box]\\n\"\n\n    rules = extract_rules(root_node, feature_names)\n    dot_data += rules\n\n    dot_data += \"}\"\n    return dot_data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.586240Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.586614Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.601762Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.586584Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.600874Z\"}}\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\n\ndef cramers_v(df):\n    result = pd.DataFrame(index=df.columns, columns=df.columns)\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                contingency_table = pd.crosstab(df[col1], df[col2])\n                X2 = stats.chi2_contingency(contingency_table, correction=False)[0]\n                n = np.sum(contingency_table.to_numpy())\n                minDim = min(contingency_table.shape)-1\n                V = np.sqrt((X2/n) / minDim)\n                result.loc[col1, col2] = V\n    return result\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.603236Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.603631Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.614613Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.603600Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.613574Z\"}}\nimport pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif_numeric(df):\n    # calculate VIF for each predictor variable\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n\n    return vif_data\n\ndef calculate_vif_categorical(df,verbose=0):\n    # create dummy variables for categorical variables\n    df_dummies = pd.get_dummies(df, drop_first=True)\n    if(verbose!=0):\n        print(df_dummies.dtypes)\n    # calculate VIF for each predictor variable\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df_dummies.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df_dummies.values, i) for i in range(len(df_dummies.columns))]\n\n    return vif_data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.617529Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.618106Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.630429Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.618070Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.629426Z\"}}\nfrom pyspark.sql.functions import col\n\ndef selectByTypePyspark(colType, df):\n    cols = [field.name for field in df.schema.fields if field.dataType == colType]\n    return df.select(*cols)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.631876Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.632364Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.643310Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.632306Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.641976Z\"}}\ndef deselectByTypePyspark(colType, df):\n    cols = [field.name for field in df.schema.fields if field.dataType != colType]\n    return df.select(*cols)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.644773Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.645208Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.658415Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.645167Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.657442Z\"}}\nimport re\n\ndef parse_tree_string(tree_string):\n    # Split the tree string into lines\n    lines = tree_string.split('\\n')\n    \n    # Initialize variables\n    nodes = []\n    stack = []\n    \n    # Iterate over the lines\n    for line in lines:\n        # Skip the first line\n        if line.startswith('DecisionTreeClassificationModel'):\n            continue\n        \n        # Find the depth of the current node\n        depth = line.find('If')\n        \n        # Check if the line represents a decision node\n        if depth >= 0:\n            # Find the feature and threshold of the current node\n            feature, threshold = re.findall(r'feature (\\d+) <= ([\\d.]+)', line)[0]\n            \n            # Create a new node\n            node = {'feature': int(feature), 'threshold': float(threshold), 'left': None, 'right': None}\n        else:\n            # Find the prediction of the current node\n            prediction = re.findall(r'Predict: ([\\d.]+)', line)[0]\n            \n            # Create a new node\n            node = {'prediction': float(prediction)}\n            \n            # Set the depth of the current node\n            depth = len(stack) - 1\n        \n        # Add the node to the list of nodes\n        nodes.append(node)\n        \n        # Update the stack\n        while len(stack) > depth:\n            stack.pop()\n        \n        # Update the parent node\n        if len(stack) > 0:\n            parent_node = nodes[stack[-1]]\n            if parent_node['left'] is None:\n                parent_node['left'] = len(nodes) - 1\n            else:\n                parent_node['right'] = len(nodes) - 1\n        \n        # Add the current node to the stack\n        stack.append(len(nodes) - 1)\n    \n    # Return the root node\n    return nodes[0]\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.659528Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.659870Z\",\"iopub.status.idle\":\"2023-07-02T10:56:29.677041Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.659841Z\",\"shell.execute_reply\":\"2023-07-02T10:56:29.675698Z\"}}\nfrom graphviz import Digraph\n\ndef plot_tree(node, graph=None, parent=None, edge_label=None):\n    # Create a new graph\n    if graph is None:\n        graph = Digraph()\n    \n    # Get the node ID\n    node_id = str(id(node))\n    \n    # Add the node to the graph\n    if 'prediction' in node:\n        graph.node(node_id, label=str(node['prediction']))\n    else:\n        graph.node(node_id, label=f'X[{node[\"feature\"]}] <= {node[\"threshold\"]}')\n    \n    # Add the edge to the graph\n    if parent is not None:\n        graph.edge(parent, node_id, label=edge_label)\n    \n    # Recursively plot the child nodes\n    if 'left' in node:\n        plot_tree(node['left'], graph=graph, parent=node_id, edge_label='True')\n    if 'right' in node:\n        plot_tree(node['right'], graph=graph, parent=node_id, edge_label='False')\n    \n    # Return the graph\n    return graph\n\n# %% [markdown]\n# # Input Data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:29.679094Z\",\"iopub.execute_input\":\"2023-07-02T10:56:29.679594Z\",\"iopub.status.idle\":\"2023-07-02T10:56:39.155728Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:29.679547Z\",\"shell.execute_reply\":\"2023-07-02T10:56:39.154227Z\"}}\nloan_df = pd.read_csv('/kaggle/input/rakamin-idx/loan_data_2007_2014.csv',low_memory=False)\nloan_df.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T10:56:39.158946Z\",\"iopub.execute_input\":\"2023-07-02T10:56:39.159409Z\",\"iopub.status.idle\":\"2023-07-02T11:07:11.269308Z\",\"shell.execute_reply.started\":\"2023-07-02T10:56:39.159290Z\",\"shell.execute_reply\":\"2023-07-02T11:07:11.268093Z\"}}\nfrom ydata_profiling import ProfileReport\nprofile = ProfileReport(loan_df, title=\"Profiling Report\")\nprofile.to_file(\"your_report.html\")\n\n# %% [markdown]\n# # Cleaning\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:11.270942Z\",\"iopub.execute_input\":\"2023-07-02T11:07:11.271316Z\",\"iopub.status.idle\":\"2023-07-02T11:07:11.431279Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:11.271284Z\",\"shell.execute_reply\":\"2023-07-02T11:07:11.430171Z\"}}\nprint(loan_df.iloc[:,:50].dtypes)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:11.432632Z\",\"iopub.execute_input\":\"2023-07-02T11:07:11.432963Z\",\"iopub.status.idle\":\"2023-07-02T11:07:11.464579Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:11.432934Z\",\"shell.execute_reply\":\"2023-07-02T11:07:11.463238Z\"}}\nprint(loan_df.iloc[:,50:].dtypes)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:11.467067Z\",\"iopub.execute_input\":\"2023-07-02T11:07:11.467476Z\",\"iopub.status.idle\":\"2023-07-02T11:07:14.528714Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:11.467442Z\",\"shell.execute_reply\":\"2023-07-02T11:07:14.527574Z\"}}\nmissing_value_bar = loan_df.isna().sum()[loan_df.isna().sum()>0] / loan_df.shape[0] * 100 \nmissing_value_bar = missing_value_bar.sort_values()\nmissing_value_bar\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:14.530275Z\",\"iopub.execute_input\":\"2023-07-02T11:07:14.530744Z\",\"iopub.status.idle\":\"2023-07-02T11:07:15.116660Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:14.530703Z\",\"shell.execute_reply\":\"2023-07-02T11:07:15.115505Z\"}}\nax = missing_value_bar.plot(kind='barh',figsize=(20,7))\nax.axvline(x=50, color='r')\nplt.show()\n\n# %% [markdown]\n# Minimal 50%\n# https://www.sciencedirect.com/science/article/pii/S0895435618308710\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:15.118172Z\",\"iopub.execute_input\":\"2023-07-02T11:07:15.118740Z\",\"iopub.status.idle\":\"2023-07-02T11:07:15.489092Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:15.118707Z\",\"shell.execute_reply\":\"2023-07-02T11:07:15.487820Z\"}}\nlist_col_miss = missing_value_bar[missing_value_bar>50].index.tolist()\nloan_df = loan_df.drop(list_col_miss,axis=1)\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:15.490859Z\",\"iopub.execute_input\":\"2023-07-02T11:07:15.491224Z\",\"iopub.status.idle\":\"2023-07-02T11:07:15.496047Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:15.491194Z\",\"shell.execute_reply\":\"2023-07-02T11:07:15.494759Z\"}}\n# reject_stats = ['url',\n#  'zip_code',\n#  'title',\n#  'mths_since_last_record',\n#  'mths_since_last_delinq',\n#  'issue_d',\n#  'inq_last_6mths',\n#  'emp_title',\n#  'emp_length',\n#  'earliest_cr_line',\n#  'desc',\n#  'addr_state'\n# ]\n\n# loan_df = loan_df.drop(reject_stats,axis=1)\n# loan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:15.497605Z\",\"iopub.execute_input\":\"2023-07-02T11:07:15.498083Z\",\"iopub.status.idle\":\"2023-07-02T11:07:15.541562Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:15.498051Z\",\"shell.execute_reply\":\"2023-07-02T11:07:15.540401Z\"}}\nloan_df.term.unique()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:15.542954Z\",\"iopub.execute_input\":\"2023-07-02T11:07:15.543283Z\",\"iopub.status.idle\":\"2023-07-02T11:07:15.619697Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:15.543256Z\",\"shell.execute_reply\":\"2023-07-02T11:07:15.618397Z\"}}\nloan_df.term.replace({' 36 months':'36 months',\n                      ' 60 months':'60 months'},\n                     inplace=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:15.621139Z\",\"iopub.execute_input\":\"2023-07-02T11:07:15.623042Z\",\"iopub.status.idle\":\"2023-07-02T11:07:16.079493Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:15.623000Z\",\"shell.execute_reply\":\"2023-07-02T11:07:16.078413Z\"}}\nloan_df = loan_df.drop(\"Unnamed: 0\",axis=1)\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:16.080959Z\",\"iopub.execute_input\":\"2023-07-02T11:07:16.081647Z\",\"iopub.status.idle\":\"2023-07-02T11:07:16.271189Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:16.081613Z\",\"shell.execute_reply\":\"2023-07-02T11:07:16.269985Z\"}}\nloan_df = loan_df.drop(['id','member_id'],axis=1)\nloan_df.head()\n\n# %% [markdown]\n# # Delete 1 value only\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:16.273017Z\",\"iopub.execute_input\":\"2023-07-02T11:07:16.273502Z\",\"iopub.status.idle\":\"2023-07-02T11:07:17.734021Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:16.273459Z\",\"shell.execute_reply\":\"2023-07-02T11:07:17.732859Z\"}}\nfor colname in loan_df.columns:\n    banyak = len(loan_df[colname].unique())\n    if(banyak==1):\n        print(colname,banyak)\nloan_df = loan_df.drop(['policy_code','application_type'],axis=1)\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:17.735724Z\",\"iopub.execute_input\":\"2023-07-02T11:07:17.736160Z\",\"iopub.status.idle\":\"2023-07-02T11:07:17.765042Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:17.736117Z\",\"shell.execute_reply\":\"2023-07-02T11:07:17.763739Z\"}}\nloan_df.head()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:17.774042Z\",\"iopub.execute_input\":\"2023-07-02T11:07:17.774437Z\",\"iopub.status.idle\":\"2023-07-02T11:07:17.785301Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:17.774406Z\",\"shell.execute_reply\":\"2023-07-02T11:07:17.784069Z\"}}\nloan_df.dtypes\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:17.786780Z\",\"iopub.execute_input\":\"2023-07-02T11:07:17.787105Z\",\"iopub.status.idle\":\"2023-07-02T11:07:19.265526Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:17.787079Z\",\"shell.execute_reply\":\"2023-07-02T11:07:19.264100Z\"}}\ncol_drop_uniq = []\nfor i in loan_df.select_dtypes(include=['object']).columns:\n    print(i)\n    print(loan_df[i].unique())\n    len_col = len(loan_df[i].unique())\n    print(len_col)\n    if(len_col>50):\n        col_drop_uniq.append(i)\n    print()\nprint(col_drop_uniq)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:19.267090Z\",\"iopub.execute_input\":\"2023-07-02T11:07:19.267581Z\",\"iopub.status.idle\":\"2023-07-02T11:07:19.274151Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:19.267544Z\",\"shell.execute_reply\":\"2023-07-02T11:07:19.273243Z\"}}\ncol_drop_uniq = ['emp_title','url','title','zip_code']\ncol_drop_uniq\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:19.275319Z\",\"iopub.execute_input\":\"2023-07-02T11:07:19.275674Z\",\"iopub.status.idle\":\"2023-07-02T11:07:19.755715Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:19.275645Z\",\"shell.execute_reply\":\"2023-07-02T11:07:19.754318Z\"}}\nloan_df = loan_df.drop(col_drop_uniq,axis=1)\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:19.757718Z\",\"iopub.execute_input\":\"2023-07-02T11:07:19.758189Z\",\"iopub.status.idle\":\"2023-07-02T11:07:20.870549Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:19.758143Z\",\"shell.execute_reply\":\"2023-07-02T11:07:20.869418Z\"}}\nloan_df.isna().sum()\n\n# %% [markdown]\n# # Imputation\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:20.872017Z\",\"iopub.execute_input\":\"2023-07-02T11:07:20.873150Z\",\"iopub.status.idle\":\"2023-07-02T11:07:21.605275Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:20.873109Z\",\"shell.execute_reply\":\"2023-07-02T11:07:21.601499Z\"}}\nloan_df.last_pymnt_d = pd.to_datetime(loan_df.last_pymnt_d,format=\"%b-%y\")\nloan_df.last_credit_pull_d = pd.to_datetime(loan_df.last_credit_pull_d,format=\"%b-%y\")\nloan_df.next_pymnt_d = pd.to_datetime(loan_df.next_pymnt_d,format=\"%b-%y\")\nloan_df.earliest_cr_line = pd.to_datetime(loan_df.earliest_cr_line,format=\"%b-%y\")\nloan_df.issue_d = pd.to_datetime(loan_df.issue_d,format=\"%b-%y\")\nloan_df.dtypes\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:21.610004Z\",\"iopub.execute_input\":\"2023-07-02T11:07:21.611226Z\",\"iopub.status.idle\":\"2023-07-02T11:07:21.816290Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:21.611118Z\",\"shell.execute_reply\":\"2023-07-02T11:07:21.810417Z\"}}\nlist_col_date = loan_df.select_dtypes(include=['datetime64']).columns.tolist()\nloan_df[list_col_date] = loan_df[list_col_date].interpolate(method='pad',)\nloan_df.next_pymnt_d = loan_df.next_pymnt_d.fillna(loan_df.next_pymnt_d.value_counts().index[0])\nprint(loan_df[list_col_date].isna().sum())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:21.820344Z\",\"iopub.execute_input\":\"2023-07-02T11:07:21.821898Z\",\"iopub.status.idle\":\"2023-07-02T11:07:22.080173Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:21.821636Z\",\"shell.execute_reply\":\"2023-07-02T11:07:22.076517Z\"}}\nloan_df.select_dtypes(include='object')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:07:22.086785Z\",\"iopub.execute_input\":\"2023-07-02T11:07:22.088026Z\",\"iopub.status.idle\":\"2023-07-02T11:10:24.169878Z\",\"shell.execute_reply.started\":\"2023-07-02T11:07:22.087864Z\",\"shell.execute_reply\":\"2023-07-02T11:10:24.169014Z\"}}\nfrom statsmodels.imputation.mice import MICEData\nloan_df_nondate = loan_df.select_dtypes(exclude=['object','datetime64'])\nimp = MICEData(loan_df_nondate)\nimp.update_all(5)\nloan_df_nondate = imp.data\nloan_df[loan_df_nondate.columns.tolist()] = loan_df_nondate\nloan_df.emp_length = loan_df.emp_length.fillna(loan_df.emp_length.value_counts().index[0])\nloan_df.isna().sum()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:10:24.171217Z\",\"iopub.execute_input\":\"2023-07-02T11:10:24.171725Z\",\"iopub.status.idle\":\"2023-07-02T11:10:25.025259Z\",\"shell.execute_reply.started\":\"2023-07-02T11:10:24.171697Z\",\"shell.execute_reply\":\"2023-07-02T11:10:25.024164Z\"}}\nfrom statsmodels.imputation.mice import MICEData\nloan_df_num = loan_df.select_dtypes(exclude=['object','datetime64'])\nloan_df.isna().sum()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:10:25.026628Z\",\"iopub.execute_input\":\"2023-07-02T11:10:25.026971Z\",\"iopub.status.idle\":\"2023-07-02T11:10:28.516578Z\",\"shell.execute_reply.started\":\"2023-07-02T11:10:25.026934Z\",\"shell.execute_reply\":\"2023-07-02T11:10:28.515528Z\"}}\nimport seaborn as sns\n# Resize the heatmap\nfig, ax = plt.subplots(figsize=(16, 10))\nsns.heatmap(loan_df_num.corr(),\n            annot=True,\n            ax=ax,\n            annot_kws={\"fontsize\":7},\n            \n           )\n\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:10:28.517997Z\",\"iopub.execute_input\":\"2023-07-02T11:10:28.523045Z\",\"iopub.status.idle\":\"2023-07-02T11:11:24.545709Z\",\"shell.execute_reply.started\":\"2023-07-02T11:10:28.522989Z\",\"shell.execute_reply\":\"2023-07-02T11:11:24.544396Z\"}}\nnumeric_vif = calculate_vif_numeric(loan_df_num)\nnumeric_vif = numeric_vif.sort_values('VIF',ascending=False)\nnumeric_vif\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:24.548183Z\",\"iopub.execute_input\":\"2023-07-02T11:11:24.549191Z\",\"iopub.status.idle\":\"2023-07-02T11:11:39.433476Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:24.549135Z\",\"shell.execute_reply\":\"2023-07-02T11:11:39.432258Z\"}}\ndrop_vif = ['total_pymnt',\n            'out_prncp',\n            'funded_amnt',\n            'funded_amnt_inv',\n            'total_pymnt_inv',\n            'loan_amnt',\n            'installment',\n            'int_rate',\n            'open_acc',\n            'total_rec_prncp',\n            'total_acc',\n            'revol_util',\n            'revol_bal'\n            ]\nnumeric_vif = calculate_vif_numeric(loan_df_num.drop(drop_vif,\n                                                     axis=1\n                                                    )\n                                   )\nnumeric_vif = numeric_vif.sort_values('VIF',ascending=False)\nnumeric_vif\n\n# %% [markdown]\n# Multicollinearity dihilangkan agar akurasi bisa lebih bagus dan juga supaya dapat memperingkas kolom agar komputasi lebih ringan. batas multikolinear adalah 5% . https://quantifyinghealth.com/vif-threshold/\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:39.439552Z\",\"iopub.execute_input\":\"2023-07-02T11:11:39.442682Z\",\"iopub.status.idle\":\"2023-07-02T11:11:39.515785Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:39.442595Z\",\"shell.execute_reply\":\"2023-07-02T11:11:39.514546Z\"}}\nloan_df_num = loan_df_num[numeric_vif.feature.to_list()]\nloan_df_num\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:39.522855Z\",\"iopub.execute_input\":\"2023-07-02T11:11:39.526348Z\",\"iopub.status.idle\":\"2023-07-02T11:11:39.880606Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:39.526267Z\",\"shell.execute_reply\":\"2023-07-02T11:11:39.879242Z\"}}\nloan_df = loan_df.drop(drop_vif,axis=1)\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:39.882479Z\",\"iopub.execute_input\":\"2023-07-02T11:11:39.883420Z\",\"iopub.status.idle\":\"2023-07-02T11:11:52.683408Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:39.883363Z\",\"shell.execute_reply\":\"2023-07-02T11:11:52.682137Z\"}}\ncrem_v = cramers_v(loan_df.select_dtypes(include='object'))\ncrem_v = crem_v.fillna(1)\nsns.heatmap(crem_v,annot=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:52.685143Z\",\"iopub.execute_input\":\"2023-07-02T11:11:52.685595Z\",\"iopub.status.idle\":\"2023-07-02T11:11:53.021748Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:52.685553Z\",\"shell.execute_reply\":\"2023-07-02T11:11:53.020205Z\"}}\nloan_df = loan_df.drop(['sub_grade','addr_state'],\n                       axis=1\n                      )\nloan_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:11:53.023079Z\",\"iopub.execute_input\":\"2023-07-02T11:11:53.023410Z\",\"iopub.status.idle\":\"2023-07-02T11:15:10.660917Z\",\"shell.execute_reply.started\":\"2023-07-02T11:11:53.023383Z\",\"shell.execute_reply\":\"2023-07-02T11:15:10.659788Z\"}}\n# objek_vif = calculate_vif_categorical(loan_df.select_dtypes(include='object').drop(['sub_grade'],axis=1))\nobjek_vif = calculate_vif_categorical(loan_df.select_dtypes(include='object'))\nobjek_vif = objek_vif.sort_values('VIF',ascending=False)\nobjek_vif\n\n# %% [markdown]\n# # EDA\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:10.662527Z\",\"iopub.execute_input\":\"2023-07-02T11:15:10.663317Z\",\"iopub.status.idle\":\"2023-07-02T11:15:11.190166Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:10.663275Z\",\"shell.execute_reply\":\"2023-07-02T11:15:11.189048Z\"}}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(data=loan_df,\n              y='loan_status',\n              order=loan_df['loan_status'].value_counts().index\n             )\n\n# Add annotations to display the count of each category\nfor p in plt.gca().patches:\n    plt.gca().text(p.get_width(), \n                   p.get_y() + p.get_height() / 2.,\n                   f'{int(p.get_width())}',\n                   va='center',\n                  )\n# Show the plot\nplt.show()\n\n# %% [markdown]\n# Dari Data ini membuktikan bahwa :\n# * Disini ada lebih banyak status utang, current ,fully paid, dan chargeoff\n# * Ada terjadinya ketidakseimbangan antar kelasnya\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:11.191693Z\",\"iopub.execute_input\":\"2023-07-02T11:15:11.192128Z\",\"iopub.status.idle\":\"2023-07-02T11:15:12.903811Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:11.192098Z\",\"shell.execute_reply\":\"2023-07-02T11:15:12.902421Z\"}}\nplt.figure(figsize=(20, 10))\nsns.heatmap(loan_df.corr(),annot=True,fmt='.2f')\n\n# %% [markdown]\n# ### Korelasi\n# * kemungkinan autocolinearity antara total_pymnt,total_pymnt_inv, total_rec_prncp\n# * kemungkinan autocolinearity antara funded_amnt, funded_amnt_inv\n# * kemungkinan autocolinearity antara installment dengan funded_amnt, funded_amnt_inv, dan loan_amnt\n# * kemungkinan autocolinearity antara out_prncp dengan out_prncp_inv\n\n# %% [markdown]\n# # Machine Learning with Sklearn without SMOTE\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:12.905538Z\",\"iopub.execute_input\":\"2023-07-02T11:15:12.905897Z\",\"iopub.status.idle\":\"2023-07-02T11:15:25.466095Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:12.905865Z\",\"shell.execute_reply\":\"2023-07-02T11:15:25.464573Z\"}}\n!pip install -U imbalanced-learn\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:25.468419Z\",\"iopub.execute_input\":\"2023-07-02T11:15:25.468922Z\",\"iopub.status.idle\":\"2023-07-02T11:15:26.401145Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:25.468875Z\",\"shell.execute_reply\":\"2023-07-02T11:15:26.399845Z\"}}\nloan_df_cop = loan_df.copy()\n# print(loan_df_cop.select_dtypes(include=['object']).columns)\nloan_df_cop = pd.get_dummies(loan_df_cop,columns=['term', 'grade','emp_length',\n                                                  'home_ownership', 'verification_status',\n                                                  #'loan_status',\n                                                  'pymnt_plan','purpose', 'initial_list_status'\n                                                 ]\n                            )\nloan_df_cop\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:26.402524Z\",\"iopub.execute_input\":\"2023-07-02T11:15:26.402884Z\",\"iopub.status.idle\":\"2023-07-02T11:15:27.638384Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:26.402853Z\",\"shell.execute_reply\":\"2023-07-02T11:15:27.637541Z\"}}\nloan_df_cop = extract_datetime(loan_df_cop,\n                               columns=loan_df_cop.select_dtypes(include=\"datetime64\").columns.tolist(),\n                               hour=False,\n                               minute=False,\n                               second=False,\n                               drop=True\n                              )\nloan_df_cop\n\n# %% [markdown]\n# ## Decision Tree\n\n# %% [markdown]\n# ### Without Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:27.639967Z\",\"iopub.execute_input\":\"2023-07-02T11:15:27.640275Z\",\"iopub.status.idle\":\"2023-07-02T11:15:52.531225Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:27.640249Z\",\"shell.execute_reply\":\"2023-07-02T11:15:52.530029Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay\n\nX = loan_df_cop.drop(['loan_status'],\n                     axis=1,\n                    )\ny = loan_df_cop['loan_status']\ndec_tree = DecisionTreeClassifier()\nX_train,X_test,y_train,y_test = train_test_split(X,\n                                                 y,\n                                                 train_size=0.7\n                                                )\ndec_tree.fit(X_train,y_train)\ny_pred = dec_tree.predict(X_test)\n\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:52.535377Z\",\"iopub.execute_input\":\"2023-07-02T11:15:52.535749Z\",\"iopub.status.idle\":\"2023-07-02T11:15:53.397638Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:52.535719Z\",\"shell.execute_reply\":\"2023-07-02T11:15:53.396423Z\"}}\nplot_feature_importance(dec_tree, X.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:53.399205Z\",\"iopub.execute_input\":\"2023-07-02T11:15:53.400320Z\",\"iopub.status.idle\":\"2023-07-02T11:15:53.412205Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:53.400279Z\",\"shell.execute_reply\":\"2023-07-02T11:15:53.411091Z\"}}\nfeat_importances = pd.Series(dec_tree.feature_importances_, index=X_train.columns.tolist())\nfeat_importances = feat_importances.sort_values()\nfeat_importances[feat_importances<0.01].index.tolist()\n\n# %% [markdown]\n# ### With Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:15:53.413845Z\",\"iopub.execute_input\":\"2023-07-02T11:15:53.415087Z\",\"iopub.status.idle\":\"2023-07-02T11:16:03.299480Z\",\"shell.execute_reply.started\":\"2023-07-02T11:15:53.415047Z\",\"shell.execute_reply\":\"2023-07-02T11:16:03.298337Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\ndec_tree = DecisionTreeClassifier()\nX_train_mi = X_train.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\nX_test_mi  = X_test.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n\ndec_tree.fit(X_train_mi,y_train)\ny_pred = dec_tree.predict(X_test_mi)\n\nplot_feature_importance(dec_tree, X_train_mi.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [markdown]\n# ## Random Forest\n\n# %% [markdown]\n# ### Without Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:16:03.301448Z\",\"iopub.execute_input\":\"2023-07-02T11:16:03.301826Z\",\"iopub.status.idle\":\"2023-07-02T11:18:29.225221Z\",\"shell.execute_reply.started\":\"2023-07-02T11:16:03.301795Z\",\"shell.execute_reply\":\"2023-07-02T11:18:29.224010Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\nplot_feature_importance(rfc, X_train.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [markdown]\n# ### With Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:18:29.227102Z\",\"iopub.execute_input\":\"2023-07-02T11:18:29.227461Z\",\"iopub.status.idle\":\"2023-07-02T11:18:29.291110Z\",\"shell.execute_reply.started\":\"2023-07-02T11:18:29.227430Z\",\"shell.execute_reply\":\"2023-07-02T11:18:29.290018Z\"}}\nfeat_importances = pd.Series(rfc.feature_importances_, index=X_train.columns.tolist())\nfeat_importances = feat_importances.sort_values()\nfeat_importances[feat_importances<0.01].index.tolist()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:18:29.292722Z\",\"iopub.execute_input\":\"2023-07-02T11:18:29.293121Z\",\"iopub.status.idle\":\"2023-07-02T11:18:40.445363Z\",\"shell.execute_reply.started\":\"2023-07-02T11:18:29.293089Z\",\"shell.execute_reply\":\"2023-07-02T11:18:40.444556Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\ndec_tree = DecisionTreeClassifier()\nX_train_mi = X_train.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\nX_test_mi  = X_test.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n\ndec_tree.fit(X_train_mi,y_train)\ny_pred = dec_tree.predict(X_test_mi)\n\nplot_feature_importance(dec_tree, X_train_mi.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [markdown]\n# # Machine Learning with Sklearn with SMOTE\n\n# %% [markdown]\n# ## Decision Tree\n\n# %% [markdown]\n# ### Without Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:18:40.446557Z\",\"iopub.execute_input\":\"2023-07-02T11:18:40.447222Z\",\"iopub.status.idle\":\"2023-07-02T11:24:53.829563Z\",\"shell.execute_reply.started\":\"2023-07-02T11:18:40.447192Z\",\"shell.execute_reply\":\"2023-07-02T11:24:53.828211Z\"}}\nfrom imblearn.over_sampling import SMOTE\n\nsmoteku = SMOTE(random_state=42)\nX = loan_df_cop.drop(['loan_status'],\n                     axis=1,\n                    )\ny = loan_df_cop['loan_status']\nX_res, y_res = smoteku.fit_resample(X, y)\nprint(X_res.shape)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:24:53.831272Z\",\"iopub.execute_input\":\"2023-07-02T11:24:53.832292Z\",\"iopub.status.idle\":\"2023-07-02T11:26:32.385129Z\",\"shell.execute_reply.started\":\"2023-07-02T11:24:53.832244Z\",\"shell.execute_reply\":\"2023-07-02T11:26:32.383717Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ndec_tree = DecisionTreeClassifier()\nX_train,X_test,y_train,y_test = train_test_split(X_res,\n                                                 y_res,\n                                                 train_size=0.7\n                                                )\ndec_tree.fit(X_train,y_train)\ny_pred = dec_tree.predict(X_test)\ny_pred\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:26:32.386608Z\",\"iopub.execute_input\":\"2023-07-02T11:26:32.386946Z\",\"iopub.status.idle\":\"2023-07-02T11:27:19.919985Z\",\"shell.execute_reply.started\":\"2023-07-02T11:26:32.386918Z\",\"shell.execute_reply\":\"2023-07-02T11:27:19.918793Z\"}}\nfrom sklearn.metrics import confusion_matrix,classification_report,ConfusionMatrixDisplay\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:27:19.921784Z\",\"iopub.execute_input\":\"2023-07-02T11:27:19.922137Z\",\"iopub.status.idle\":\"2023-07-02T11:27:20.761150Z\",\"shell.execute_reply.started\":\"2023-07-02T11:27:19.922098Z\",\"shell.execute_reply\":\"2023-07-02T11:27:20.759976Z\"}}\nplot_feature_importance(dec_tree, X_res.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:27:20.762849Z\",\"iopub.execute_input\":\"2023-07-02T11:27:20.763198Z\",\"iopub.status.idle\":\"2023-07-02T11:27:20.774587Z\",\"shell.execute_reply.started\":\"2023-07-02T11:27:20.763167Z\",\"shell.execute_reply\":\"2023-07-02T11:27:20.773192Z\"}}\nfeat_importances = pd.Series(dec_tree.feature_importances_, index=X_res.columns.tolist())\nfeat_importances = feat_importances.sort_values()\nfeat_importances[feat_importances<0.01].index.tolist()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:27:20.776357Z\",\"iopub.execute_input\":\"2023-07-02T11:27:20.776786Z\",\"iopub.status.idle\":\"2023-07-02T11:28:53.146417Z\",\"shell.execute_reply.started\":\"2023-07-02T11:27:20.776754Z\",\"shell.execute_reply\":\"2023-07-02T11:28:53.145133Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\ndec_tree = DecisionTreeClassifier()\n# X_train,X_test,y_train,y_test = train_test_split(X_res_mi,\n#                                                  y_res,\n#                                                  train_size=0.7\n#                                                 )\nX_train_mi = X_train.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\nX_test_mi  = X_test.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n\ndec_tree.fit(X_train_mi,y_train)\ny_pred = dec_tree.predict(X_test_mi)\n\nplot_feature_importance(dec_tree, X_train_mi.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# clf = DecisionTreeClassifier(random_state=0)\n# scoring = ['f1_macro']\n# scores = cross_validate(clf, X_res_mi, y_res, scoring=scoring, return_estimator=True)\n\n# for estimator in scores['estimator']:\n#     y_pred = estimator.predict(X_res_mi)\n#     print(estimator)\n#     print(classification_report(y_res, y_pred))\n\n# # print(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [markdown]\n# ## Random Forest\n\n# %% [markdown]\n# ### Without filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:28:53.148277Z\",\"iopub.execute_input\":\"2023-07-02T11:28:53.149458Z\",\"iopub.status.idle\":\"2023-07-02T11:45:41.655682Z\",\"shell.execute_reply.started\":\"2023-07-02T11:28:53.149412Z\",\"shell.execute_reply\":\"2023-07-02T11:45:41.654747Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\nrfc = RandomForestClassifier()\n# X_train,X_test,y_train,y_test = train_test_split(X_res_mi,\n#                                                  y_res,\n#                                                  train_size=0.7\n#                                                 )\n# X_train_mi = X_train.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n# X_test_mi  = X_test.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\n\nplot_feature_importance(rfc, X_train.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# clf = DecisionTreeClassifier(random_state=0)\n# scoring = ['f1_macro']\n# scores = cross_validate(clf, X_res_mi, y_res, scoring=scoring, return_estimator=True)\n\n# for estimator in scores['estimator']:\n#     y_pred = estimator.predict(X_res_mi)\n#     print(estimator)\n#     print(classification_report(y_res, y_pred))\n\n# # print(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:45:41.656958Z\",\"iopub.execute_input\":\"2023-07-02T11:45:41.658010Z\",\"iopub.status.idle\":\"2023-07-02T11:45:41.867485Z\",\"shell.execute_reply.started\":\"2023-07-02T11:45:41.657974Z\",\"shell.execute_reply\":\"2023-07-02T11:45:41.866184Z\"}}\nfeat_importances = pd.Series(rfc.feature_importances_, index=X_train.columns.tolist())\nfeat_importances = feat_importances.sort_values()\nfeat_importances[feat_importances<0.01].index.tolist()\n\n# %% [markdown]\n# ### With Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T11:45:41.869365Z\",\"iopub.execute_input\":\"2023-07-02T11:45:41.869850Z\",\"iopub.status.idle\":\"2023-07-02T12:02:38.890439Z\",\"shell.execute_reply.started\":\"2023-07-02T11:45:41.869818Z\",\"shell.execute_reply\":\"2023-07-02T12:02:38.889512Z\"}}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_validate\n\nfrom sklearn.metrics import classification_report\nrfc = RandomForestClassifier()\nX_train_mi = X_train.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\nX_test_mi  = X_test.drop(feat_importances[feat_importances<0.01].index.tolist(),axis=1)\n\nrfc.fit(X_train_mi,y_train)\ny_pred = rfc.predict(X_test_mi)\n\nplot_feature_importance(rfc, X_train_mi.columns.tolist(), figsize=(8, 12), orientation='horizontal',ascending=False)\nConfusionMatrixDisplay(confusion_matrix(y_pred=y_pred,y_true=y_test)).plot()\nprint(classification_report(y_pred=y_pred,y_true=y_test))\n\n# %% [markdown]\n# # Machine Learning With Pyspark\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:02:38.901979Z\",\"iopub.execute_input\":\"2023-07-02T12:02:38.902645Z\",\"iopub.status.idle\":\"2023-07-02T12:02:38.907107Z\",\"shell.execute_reply.started\":\"2023-07-02T12:02:38.902609Z\",\"shell.execute_reply\":\"2023-07-02T12:02:38.906192Z\"}}\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:02:38.908628Z\",\"iopub.execute_input\":\"2023-07-02T12:02:38.909018Z\",\"iopub.status.idle\":\"2023-07-02T12:07:09.186096Z\",\"shell.execute_reply.started\":\"2023-07-02T12:02:38.908985Z\",\"shell.execute_reply\":\"2023-07-02T12:07:09.184591Z\"}}\nspark = SparkSession.builder.appName(\"PandasToPySpark\").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\nspark_df = spark.createDataFrame(loan_df_cop)\nspark_df\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:07:09.188700Z\",\"iopub.execute_input\":\"2023-07-02T12:07:09.189180Z\",\"iopub.status.idle\":\"2023-07-02T12:07:09.230086Z\",\"shell.execute_reply.started\":\"2023-07-02T12:07:09.189133Z\",\"shell.execute_reply\":\"2023-07-02T12:07:09.228778Z\"}}\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\n# cat_col = ['term',\n#            'grade',\n#            'home_ownership',\n#            'verification_status',\n           #'loan_status',\n#            'pymnt_plan',\n#            'purpose',\n#            'initial_list_status']\n# One-hot encode the categorical variables\n# indexers = [StringIndexer(inputCol=colu, outputCol=colu+\"_index\") for colu in cat_col]\n# encoders = [OneHotEncoder(inputCol=colu+\"_index\", outputCol=colu+\"_vec\") for colu in cat_col]\n# label_indexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_status_index\")\n# pipeline = Pipeline(stages=indexers + encoders + [label_indexer])\n# spark_df = pipeline.fit(spark_df).transform(spark_df)\nspark_df.printSchema()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:07:09.232208Z\",\"iopub.execute_input\":\"2023-07-02T12:07:09.232966Z\",\"iopub.status.idle\":\"2023-07-02T12:08:35.567620Z\",\"shell.execute_reply.started\":\"2023-07-02T12:07:09.232922Z\",\"shell.execute_reply\":\"2023-07-02T12:08:35.566420Z\"}}\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nspark_df_cop = spark_df\n# spark_df_cop = spark_df_cop.repartition(numPartitions=100)\nfeature_list = spark_df_cop.columns\nfeature_list.remove('loan_status')\nfeature_list\n\nvec = VectorAssembler(inputCols=feature_list, outputCol = \"features\")\nspark_df_cop = vec.transform(spark_df_cop)\n\nindexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_status_index\")\nindexer_fit = indexer.fit(spark_df_cop)\nspark_df_cop = indexer_fit.transform(spark_df_cop)\n\ntrain, test = spark_df_cop.randomSplit([0.8, 0.2], seed=42)\n\nrf = RandomForestClassifier(labelCol=\"loan_status_index\", featuresCol=\"features\", numTrees=100)\npipeline = Pipeline(stages=[rf])\nmodel = pipeline.fit(train)\n\n# Make predictions\npredictions = model.transform(test)\n\n# Evaluate model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_status_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Accuracy = %g\" % accuracy)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:35.568877Z\",\"iopub.execute_input\":\"2023-07-02T12:08:35.569259Z\",\"iopub.status.idle\":\"2023-07-02T12:08:44.875317Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:35.569227Z\",\"shell.execute_reply\":\"2023-07-02T12:08:44.874049Z\"}}\n# Generate classification report\nlabels = indexer_fit.labels # Get the label names from the indexer\npredictions_df = predictions.select(\"loan_status_index\", \"prediction\").toPandas() # Convert predictions to pandas dataframe\nreport_df = pd.crosstab(predictions_df[\"loan_status_index\"], predictions_df[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"]) # Create a confusion matrix\n# report_df.index = labels # Replace the numeric indices with label names\n# report_df.columns = labels # Replace the numeric indices with label names\nprint(report_df) # Print the classification report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:44.877423Z\",\"iopub.execute_input\":\"2023-07-02T12:08:44.878211Z\",\"iopub.status.idle\":\"2023-07-02T12:08:44.932382Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:44.878161Z\",\"shell.execute_reply\":\"2023-07-02T12:08:44.931142Z\"}}\nmy_dict = {i: indexer_fit.labels[i] for i in range(len(indexer_fit.labels))}\nmy_dict\n\n# %% [markdown]\n# Predicted is not good , because predicted and Actual not same element\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:44.934013Z\",\"iopub.execute_input\":\"2023-07-02T12:08:44.934489Z\",\"iopub.status.idle\":\"2023-07-02T12:08:52.348273Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:44.934448Z\",\"shell.execute_reply\":\"2023-07-02T12:08:52.347398Z\"}}\n# Plot feature importance\nrf_model = model.stages[0] # Get the random forest model from the pipeline\nfeature_importance = rf_model.featureImportances # Get the feature importance vector\nfeature_importance_df = pd.DataFrame(feature_importance.toArray(), index=feature_list, columns=[\"importance\"]) # Create a pandas dataframe with feature names and importance values\nfeature_importance_df.sort_values(by=\"importance\", ascending=True, inplace=True) # Sort the dataframe by importance values\nfeature_importance_df.plot(kind=\"barh\", figsize=(12, 12)) # Plot the dataframe as a bar chart\nplt.title(\"Feature Importance for Random Forest Classifier\") # Add a title to the plot\nplt.show() # Show the plot\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:52.349701Z\",\"iopub.execute_input\":\"2023-07-02T12:08:52.350254Z\",\"iopub.status.idle\":\"2023-07-02T12:08:54.175388Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:52.350221Z\",\"shell.execute_reply\":\"2023-07-02T12:08:54.174033Z\"}}\nloan_df_cop_smote = X_res.merge(y_res.to_frame(),left_index=True,right_index=True)\nloan_df_cop_smote\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:54.176989Z\",\"iopub.execute_input\":\"2023-07-02T12:08:54.177352Z\",\"iopub.status.idle\":\"2023-07-02T12:08:54.488624Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:54.177308Z\",\"shell.execute_reply\":\"2023-07-02T12:08:54.487465Z\"}}\nspark.stop()\ndel spark\n\n# %% [markdown]\n# # Spark Smote\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:08:54.490371Z\",\"iopub.execute_input\":\"2023-07-02T12:08:54.490728Z\",\"iopub.status.idle\":\"2023-07-02T12:27:40.581347Z\",\"shell.execute_reply.started\":\"2023-07-02T12:08:54.490698Z\",\"shell.execute_reply\":\"2023-07-02T12:27:40.579928Z\"}}\nspark = SparkSession.builder.appName(\"PandasToPySpark_2\").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\nspark_df_smote = spark.createDataFrame(loan_df_cop_smote)\nspark_df_smote\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:27:40.582858Z\",\"iopub.execute_input\":\"2023-07-02T12:27:40.583187Z\",\"iopub.status.idle\":\"2023-07-02T12:32:36.325569Z\",\"shell.execute_reply.started\":\"2023-07-02T12:27:40.583160Z\",\"shell.execute_reply\":\"2023-07-02T12:32:36.324388Z\"}}\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\n\nspark_df_cop = spark_df_smote\n# spark_df_cop = spark_df_cop.repartition(numPartitions=100)\nfeature_list = spark_df_cop.columns\nfeature_list.remove('loan_status')\nfeature_list\n\nvec = VectorAssembler(inputCols=feature_list, outputCol = \"features\")\nspark_df_cop = vec.transform(spark_df_cop)\n\nindexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_status_index\")\nindexer_fit = indexer.fit(spark_df_cop)\nspark_df_cop = indexer_fit.transform(spark_df_cop)\n\ntrain, test = spark_df_cop.randomSplit([0.8, 0.2], seed=42)\n\nrf = RandomForestClassifier(labelCol=\"loan_status_index\", featuresCol=\"features\", numTrees=100)\npipeline = Pipeline(stages=[rf])\nmodel = pipeline.fit(train)\n\n# Make predictions\npredictions = model.transform(test)\n\n# Evaluate model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_status_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Accuracy = %g\" % accuracy)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:32:36.326820Z\",\"iopub.execute_input\":\"2023-07-02T12:32:36.327156Z\",\"iopub.status.idle\":\"2023-07-02T12:33:15.842354Z\",\"shell.execute_reply.started\":\"2023-07-02T12:32:36.327128Z\",\"shell.execute_reply\":\"2023-07-02T12:33:15.841192Z\"}}\n# Generate classification report\nlabels = indexer_fit.labels # Get the label names from the indexer\npredictions_df = predictions.select(\"loan_status_index\", \"prediction\").toPandas() # Convert predictions to pandas dataframe\nreport_df = pd.crosstab(predictions_df[\"loan_status_index\"], predictions_df[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"]) # Create a confusion matrix\nmy_dict = {i: indexer_fit.labels[i] for i in range(len(indexer_fit.labels))}\nprint(my_dict)\nprint()\n#report_df.index = labels # Replace the numeric indices with label names\n#report_df.columns = labels # Replace the numeric indices with label names\nprint(report_df) # Print the classification report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:33:15.843656Z\",\"iopub.execute_input\":\"2023-07-02T12:33:15.844061Z\",\"iopub.status.idle\":\"2023-07-02T12:33:16.932526Z\",\"shell.execute_reply.started\":\"2023-07-02T12:33:15.844031Z\",\"shell.execute_reply\":\"2023-07-02T12:33:16.931136Z\"}}\n# Plot feature importance\nrf_model = model.stages[0] # Get the random forest model from the pipeline\nfeature_importance = rf_model.featureImportances # Get the feature importance vector\nfeature_importance_df = pd.DataFrame(feature_importance.toArray(), index=feature_list, columns=[\"importance\"]) # Create a pandas dataframe with feature names and importance values\nfeature_importance_df.sort_values(by=\"importance\", ascending=True, inplace=True) # Sort the dataframe by importance values\nfeature_importance_df.plot(kind=\"barh\", figsize=(12, 12)) # Plot the dataframe as a bar chart\nplt.title(\"Feature Importance for Random Forest Classifier\") # Add a title to the plot\nplt.show() # Show the plot\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:33:16.934001Z\",\"iopub.execute_input\":\"2023-07-02T12:33:16.934370Z\",\"iopub.status.idle\":\"2023-07-02T12:33:16.944511Z\",\"shell.execute_reply.started\":\"2023-07-02T12:33:16.934340Z\",\"shell.execute_reply\":\"2023-07-02T12:33:16.941468Z\"}}\nfeature_importance_df_ = feature_importance_df[feature_importance_df.importance > 0.01]\nfeature_importance_list_df_1per = feature_importance_df_.index.tolist()\nfeature_importance_list_df_1per\n\n# %% [markdown]\n# ## Filter Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:33:16.946506Z\",\"iopub.execute_input\":\"2023-07-02T12:33:16.946941Z\",\"iopub.status.idle\":\"2023-07-02T12:36:50.962113Z\",\"shell.execute_reply.started\":\"2023-07-02T12:33:16.946911Z\",\"shell.execute_reply\":\"2023-07-02T12:36:50.960862Z\"}}\nspark_df_cop = spark_df_smote\nfeature_list = feature_importance_list_df_1per\n\nvec = VectorAssembler(inputCols=feature_list, outputCol = \"features\")\nspark_df_cop = vec.transform(spark_df_cop)\n\nindexer = StringIndexer(inputCol=\"loan_status\", outputCol=\"loan_status_index\")\nindexer_fit = indexer.fit(spark_df_cop)\nspark_df_cop = indexer_fit.transform(spark_df_cop)\n\ntrain, test = spark_df_cop.randomSplit([0.8, 0.2], seed=42)\n\nrf = RandomForestClassifier(labelCol=\"loan_status_index\", featuresCol=\"features\", numTrees=100)\npipeline = Pipeline(stages=[rf])\nmodel = pipeline.fit(train)\n\n# Make predictions\npredictions = model.transform(test)\n\n# Evaluate model\nevaluator = MulticlassClassificationEvaluator(labelCol=\"loan_status_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Accuracy = %g\" % accuracy)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:36:50.963967Z\",\"iopub.execute_input\":\"2023-07-02T12:36:50.964447Z\",\"iopub.status.idle\":\"2023-07-02T12:37:24.036574Z\",\"shell.execute_reply.started\":\"2023-07-02T12:36:50.964402Z\",\"shell.execute_reply\":\"2023-07-02T12:37:24.035728Z\"}}\n# Generate classification report\nlabels = indexer_fit.labels # Get the label names from the indexer\npredictions_df = predictions.select(\"loan_status_index\", \"prediction\").toPandas() # Convert predictions to pandas dataframe\nreport_df = pd.crosstab(predictions_df[\"loan_status_index\"], predictions_df[\"prediction\"], rownames=[\"Actual\"], colnames=[\"Predicted\"]) # Create a confusion matrix\nmy_dict = {i: indexer_fit.labels[i] for i in range(len(indexer_fit.labels))}\nprint(my_dict)\nprint()\n#report_df.index = labels # Replace the numeric indices with label names\n#report_df.columns = labels # Replace the numeric indices with label names\nprint(report_df) # Print the classification report\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-07-02T12:37:24.040450Z\",\"iopub.execute_input\":\"2023-07-02T12:37:24.040837Z\",\"iopub.status.idle\":\"2023-07-02T12:37:24.481021Z\",\"shell.execute_reply.started\":\"2023-07-02T12:37:24.040805Z\",\"shell.execute_reply\":\"2023-07-02T12:37:24.477517Z\"}}\n# Plot feature importance\nrf_model = model.stages[0] # Get the random forest model from the pipeline\nfeature_importance = rf_model.featureImportances # Get the feature importance vector\nfeature_importance_df = pd.DataFrame(feature_importance.toArray(), index=feature_list, columns=[\"importance\"]) # Create a pandas dataframe with feature names and importance values\nfeature_importance_df.sort_values(by=\"importance\", ascending=True, inplace=True) # Sort the dataframe by importance values\nfeature_importance_df.plot(kind=\"barh\", figsize=(12, 12)) # Plot the dataframe as a bar chart\nplt.title(\"Feature Importance for Random Forest Classifier\") # Add a title to the plot\nplt.show() # Show the plot","metadata":{"_uuid":"c6547e20-e946-4fc8-a9de-0a773d8e4514","_cell_guid":"9e2bdf54-bb38-4d46-815e-64dfbf9255d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}